{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection with Explainable AI\n",
    "## Training Models with LIME and SHAP Integration\n",
    "\n",
    "This notebook demonstrates how to train SMS spam detection models with proper explainable AI capabilities using LIME and SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Explainable AI libraries\n",
    "try:\n",
    "    import lime\n",
    "    import lime.lime_text\n",
    "    LIME_AVAILABLE = True\n",
    "    print(\"‚úÖ LIME available\")\n",
    "except ImportError:\n",
    "    LIME_AVAILABLE = False\n",
    "    print(\"‚ùå LIME not available - install with: pip install lime\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"‚úÖ SHAP available\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"‚ùå SHAP not available - install with: pip install shap\")\n",
    "\n",
    "print(f\"\\nüìä Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample SMS dataset (you can replace this with real data)\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample SMS dataset for training\"\"\"\n",
    "    \n",
    "    # Sample spam messages\n",
    "    spam_messages = [\n",
    "        \"FREE! Win a $1000 gift card! Click here now!\",\n",
    "        \"URGENT! Your account will be suspended. Call 555-SCAM immediately!\",\n",
    "        \"Congratulations! You've won a lottery! Send your details to claim prize!\",\n",
    "        \"Limited time offer! Get rich quick! Click this link now!\",\n",
    "        \"WINNER! You are selected for cash prize! Reply with your bank details!\",\n",
    "        \"Free money! No strings attached! Call now to claim your reward!\",\n",
    "        \"ALERT: Suspicious activity detected. Verify your account immediately!\",\n",
    "        \"You have been chosen! Win big money! Text STOP to opt out!\",\n",
    "        \"Exclusive offer! Make money from home! Limited time only!\",\n",
    "        \"URGENT: Your payment is overdue. Pay now to avoid penalties!\"\n",
    "    ] * 10  # Repeat to get more samples\n",
    "    \n",
    "    # Sample legitimate messages\n",
    "    ham_messages = [\n",
    "        \"Hi! Are we still meeting for lunch tomorrow at 12pm?\",\n",
    "        \"Thanks for the meeting today. I'll send the report by Friday.\",\n",
    "        \"Can you pick up milk on your way home? Thanks!\",\n",
    "        \"The conference call is scheduled for 3pm. Dial-in details attached.\",\n",
    "        \"Happy birthday! Hope you have a wonderful day!\",\n",
    "        \"Reminder: Doctor appointment tomorrow at 2pm.\",\n",
    "        \"Great job on the presentation! The client was impressed.\",\n",
    "        \"Movie starts at 7pm. See you at the theater!\",\n",
    "        \"Flight delayed by 30 minutes. New arrival time is 8:45pm.\",\n",
    "        \"Package delivered successfully. Thank you for your order!\"\n",
    "    ] * 10  # Repeat to get more samples\n",
    "    \n",
    "    # Create DataFrame\n",
    "    messages = spam_messages + ham_messages\n",
    "    labels = ['spam'] * len(spam_messages) + ['ham'] * len(ham_messages)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'message': messages,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load or create dataset\n",
    "df = create_sample_dataset()\n",
    "print(f\"üìä Dataset created with {len(df)} messages\")\n",
    "print(f\"üìà Distribution: {df['label'].value_counts().to_dict()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and feature engineering\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing\"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocess messages\n",
    "df['processed_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Convert labels to binary\n",
    "df['label_binary'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(\"‚úÖ Text preprocessing completed\")\n",
    "print(\"\\nSample processed messages:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df.iloc[i]['message']}\")\n",
    "    print(f\"Processed: {df.iloc[i]['processed_message']}\")\n",
    "    print(f\"Label: {df.iloc[i]['label']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and create features\n",
    "X = df['processed_message']\n",
    "y = df['label_binary']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Training set: {len(X_train)} messages\")\n",
    "print(f\"üìä Test set: {len(X_test)} messages\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit vectorizer and transform data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ TF-IDF vectorization completed\")\n",
    "print(f\"üìä Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"üìä Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    y_pred_proba = model.predict_proba(X_test_tfidf)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.3f}, CV: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nüèÜ Best model: {best_model_name} with accuracy {model_results[best_model_name]['accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretability - Feature importance for different model types\n",
    "def get_feature_importance(model, vectorizer, model_name):\n",
    "    \"\"\"Extract feature importance from different model types\"\"\"\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    if hasattr(model, 'coef_'):  # Linear models (Logistic Regression, SVM)\n",
    "        # For binary classification, coef_ shape is (1, n_features)\n",
    "        coefficients = model.coef_[0]\n",
    "        \n",
    "        # Get top positive and negative features\n",
    "        feature_importance = list(zip(feature_names, coefficients))\n",
    "        \n",
    "        # Sort by absolute importance\n",
    "        feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        \n",
    "        return feature_importance[:20]  # Top 20 features\n",
    "        \n",
    "    elif hasattr(model, 'feature_importances_'):  # Tree-based models\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        feature_importance = list(zip(feature_names, importances))\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return feature_importance[:20]  # Top 20 features\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get feature importance for best model\n",
    "feature_importance = get_feature_importance(best_model, vectorizer, best_model_name)\n",
    "\n",
    "if feature_importance:\n",
    "    print(f\"\\nüìä Top features for {best_model_name}:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance[:10], 1):\n",
    "        print(f\"{i:2d}. {feature:15s} : {importance:8.4f}\")\n",
    "        \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    features, importances = zip(*feature_importance[:15])\n",
    "    \n",
    "    plt.barh(range(len(features)), importances)\n",
    "    plt.yticks(range(len(features)), features)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Features - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME Explanations\n",
    "if LIME_AVAILABLE:\n",
    "    print(\"\\nüîç Setting up LIME explainer...\")\n",
    "    \n",
    "    # Create LIME explainer\n",
    "    lime_explainer = lime.lime_text.LimeTextExplainer(\n",
    "        class_names=['ham', 'spam'],\n",
    "        feature_selection='auto',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Create prediction function for LIME\n",
    "    def predict_fn(texts):\n",
    "        vectors = vectorizer.transform(texts)\n",
    "        return best_model.predict_proba(vectors)\n",
    "    \n",
    "    # Test LIME on sample messages\n",
    "    test_messages = [\n",
    "        \"FREE! Win a $1000 gift card! Click here now!\",\n",
    "        \"Hi! Are we still meeting for lunch tomorrow?\"\n",
    "    ]\n",
    "    \n",
    "    for i, message in enumerate(test_messages):\n",
    "        print(f\"\\nüìù LIME Analysis {i+1}: {message[:50]}...\")\n",
    "        \n",
    "        # Generate LIME explanation\n",
    "        explanation = lime_explainer.explain_instance(\n",
    "            message,\n",
    "            predict_fn,\n",
    "            num_features=10,\n",
    "            labels=[0, 1]\n",
    "        )\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction_proba = predict_fn([message])[0]\n",
    "        predicted_class = 'spam' if prediction_proba[1] > 0.5 else 'ham'\n",
    "        confidence = max(prediction_proba)\n",
    "        \n",
    "        print(f\"üéØ Prediction: {predicted_class} (confidence: {confidence:.3f})\")\n",
    "        print(f\"üìä Probabilities: ham={prediction_proba[0]:.3f}, spam={prediction_proba[1]:.3f}\")\n",
    "        \n",
    "        # Show top features\n",
    "        print(\"üîç LIME Feature Analysis:\")\n",
    "        for feature, importance in explanation.as_list():\n",
    "            direction = \"‚Üí SPAM\" if importance > 0 else \"‚Üí HAM\"\n",
    "            print(f\"   {feature:15s}: {importance:7.3f} {direction}\")\n",
    "        \n",
    "        # Save explanation as HTML (optional)\n",
    "        explanation.save_to_file(f'lime_explanation_{i+1}.html')\n",
    "        print(f\"üíæ Saved explanation to lime_explanation_{i+1}.html\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  LIME not available - install with: pip install lime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanations\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"\\nüîç Setting up SHAP explainer...\")\n",
    "    \n",
    "    try:\n",
    "        # Create appropriate SHAP explainer based on model type\n",
    "        if hasattr(best_model, 'coef_'):  # Linear models\n",
    "            # For linear models, use LinearExplainer\n",
    "            shap_explainer = shap.LinearExplainer(\n",
    "                best_model, \n",
    "                X_train_tfidf,\n",
    "                feature_perturbation=\"interventional\"\n",
    "            )\n",
    "            explainer_type = \"Linear\"\n",
    "            \n",
    "        elif hasattr(best_model, 'feature_importances_'):  # Tree-based models\n",
    "            # For tree models, use TreeExplainer\n",
    "            shap_explainer = shap.TreeExplainer(best_model)\n",
    "            explainer_type = \"Tree\"\n",
    "            \n",
    "        else:\n",
    "            # For other models, use KernelExplainer (slower but works with any model)\n",
    "            def model_predict(X):\n",
    "                return best_model.predict_proba(X)[:, 1]  # Return spam probability\n",
    "            \n",
    "            # Use a subset of training data as background\n",
    "            background = shap.sample(X_train_tfidf, 100)\n",
    "            shap_explainer = shap.KernelExplainer(model_predict, background)\n",
    "            explainer_type = \"Kernel\"\n",
    "        \n",
    "        print(f\"‚úÖ SHAP {explainer_type} explainer initialized\")\n",
    "        \n",
    "        # Test SHAP on sample messages\n",
    "        test_indices = [0, 1]  # First two test samples\n",
    "        \n",
    "        for idx in test_indices:\n",
    "            message = X_test.iloc[idx]\n",
    "            true_label = 'spam' if y_test.iloc[idx] == 1 else 'ham'\n",
    "            \n",
    "            print(f\"\\nüìù SHAP Analysis: {message[:50]}...\")\n",
    "            print(f\"üè∑Ô∏è  True label: {true_label}\")\n",
    "            \n",
    "            # Transform message\n",
    "            message_tfidf = vectorizer.transform([message])\n",
    "            \n",
    "            # Get prediction\n",
    "            prediction_proba = best_model.predict_proba(message_tfidf)[0]\n",
    "            predicted_class = 'spam' if prediction_proba[1] > 0.5 else 'ham'\n",
    "            confidence = max(prediction_proba)\n",
    "            \n",
    "            print(f\"üéØ Prediction: {predicted_class} (confidence: {confidence:.3f})\")\n",
    "            print(f\"üìä Probabilities: ham={prediction_proba[0]:.3f}, spam={prediction_proba[1]:.3f}\")\n",
    "            \n",
    "            # Generate SHAP values\n",
    "            if explainer_type == \"Kernel\":\n",
    "                shap_values = shap_explainer.shap_values(message_tfidf, nsamples=100)\n",
    "            else:\n",
    "                shap_values = shap_explainer.shap_values(message_tfidf)\n",
    "                \n",
    "            # For binary classification, get spam class values\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]  # Spam class\n",
    "            elif len(shap_values.shape) > 1 and shap_values.shape[1] > 1:\n",
    "                shap_values = shap_values[:, 1]  # Spam class\n",
    "            \n",
    "            # Get feature names and values\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Get top contributing features\n",
    "            if len(shap_values.shape) > 1:\n",
    "                feature_contributions = list(zip(feature_names, shap_values[0]))\n",
    "            else:\n",
    "                feature_contributions = list(zip(feature_names, shap_values))\n",
    "                \n",
    "            # Sort by absolute contribution\n",
    "            feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(\"üîç SHAP Feature Analysis (Top 10):\")\n",
    "            for feature, contribution in feature_contributions[:10]:\n",
    "                if abs(contribution) > 0.001:  # Only show meaningful contributions\n",
    "                    direction = \"‚Üí SPAM\" if contribution > 0 else \"‚Üí HAM\"\n",
    "                    print(f\"   {feature:15s}: {contribution:7.3f} {direction}\")\n",
    "        \n",
    "        # Create SHAP summary plot\n",
    "        print(\"\\nüìä Creating SHAP summary plot...\")\n",
    "        \n",
    "        # Get SHAP values for a subset of test data\n",
    "        test_subset = X_test_tfidf[:10]  # First 10 test samples\n",
    "        \n",
    "        if explainer_type == \"Kernel\":\n",
    "            shap_values_subset = shap_explainer.shap_values(test_subset, nsamples=50)\n",
    "        else:\n",
    "            shap_values_subset = shap_explainer.shap_values(test_subset)\n",
    "            \n",
    "        # Handle different SHAP value formats\n",
    "        if isinstance(shap_values_subset, list):\n",
    "            shap_values_subset = shap_values_subset[1]  # Spam class\n",
    "        elif len(shap_values_subset.shape) > 2:\n",
    "            shap_values_subset = shap_values_subset[:, :, 1]  # Spam class\n",
    "            \n",
    "        # Create summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(\n",
    "            shap_values_subset, \n",
    "            test_subset, \n",
    "            feature_names=vectorizer.get_feature_names_out(),\n",
    "            max_display=20,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title('SHAP Feature Importance Summary')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  SHAP analysis error: {e}\")\n",
    "        print(\"This might be due to model compatibility or data format issues.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  SHAP not available - install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and vectorizer\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model and vectorizer\n",
    "model_path = f'../models/spam_model_{best_model_name.lower().replace(\" \", \"_\")}.joblib'\n",
    "vectorizer_path = '../models/tfidf_vectorizer.joblib'\n",
    "metadata_path = '../models/model_metadata.json'\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "# Save vectorizer\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"üíæ Vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_type': type(best_model).__name__,\n",
    "    'accuracy': float(model_results[best_model_name]['accuracy']),\n",
    "    'cv_mean': float(model_results[best_model_name]['cv_mean']),\n",
    "    'cv_std': float(model_results[best_model_name]['cv_std']),\n",
    "    'feature_count': len(vectorizer.vocabulary_),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'lime_available': LIME_AVAILABLE,\n",
    "    'shap_available': SHAP_AVAILABLE,\n",
    "    'explainable_ai': True\n",
    "}\n",
    "\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "    \n",
    "print(f\"üíæ Metadata saved to: {metadata_path}\")\n",
    "print(f\"\\n‚úÖ Model training and explainable AI setup complete!\")\n",
    "print(f\"üéØ Best model: {best_model_name} with {metadata['accuracy']:.3f} accuracy\")\n",
    "print(f\"üîç Explainable AI: LIME={LIME_AVAILABLE}, SHAP={SHAP_AVAILABLE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
