{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection Model Training\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load and explore the SMS spam dataset\n",
    "2. Preprocess the text data\n",
    "3. Train a machine learning model\n",
    "4. Evaluate the model performance\n",
    "5. Save the model for use in Flask backend\n",
    "\n",
    "## Dataset\n",
    "We'll use the SMS Spam Collection dataset which contains SMS messages labeled as 'spam' or 'ham' (legitimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SMS Spam Collection dataset\n",
    "# You can download it from: https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "# Or use the sample data below\n",
    "\n",
    "# Sample data for demonstration (replace with actual dataset)\n",
    "sample_data = {\n",
    "    'label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam'] * 100,\n",
    "    'message': [\n",
    "        'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
    "        'Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&Cs apply 08452810075over18\\'s',\n",
    "        'U dun say so early hor... U c already then say...',\n",
    "        'FreeMsg Hey there darling it\\'s been 3 week\\'s now and no word back! I\\'d like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv',\n",
    "        'Even my brother is not like to speak with me. They treat me like aids patent.',\n",
    "        'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.',\n",
    "        'Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030',\n",
    "        'SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info'\n",
    "    ] * 100\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess SMS text for machine learning\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Convert labels to binary (0 for ham, 1 for spam)\n",
    "df['label_binary'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(\"\\nSample processed messages:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df['message'].iloc[i][:100]}...\")\n",
    "    print(f\"Processed: {df['processed_message'].iloc[i][:100]}...\")\n",
    "    print(f\"Label: {df['label'].iloc[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df['processed_message']\n",
    "y = df['label_binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set spam ratio: {y_train.mean():.3f}\")\n",
    "print(f\"Test set spam ratio: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit vocabulary size\n",
    "    stop_words='english',  # Remove common English stop words\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and compare performance\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_accuracy = model_results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for the best model\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Model comparison\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Model for Flask Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = '../backend/ml_model/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model and vectorizer\n",
    "model_path = os.path.join(models_dir, 'spam_model.pkl')\n",
    "vectorizer_path = os.path.join(models_dir, 'vectorizer.pkl')\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'accuracy': best_accuracy,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'features': X_train_tfidf.shape[1],\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'vectorizer_params': vectorizer.get_params()\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and test it\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "# Test with sample messages\n",
    "test_messages = [\n",
    "    \"Hi, how are you doing today?\",\n",
    "    \"FREE! Win a £1000 cash prize! Text WIN to 12345 now!\",\n",
    "    \"Can you pick up some milk on your way home?\",\n",
    "    \"URGENT! Your account will be closed. Click here immediately!\",\n",
    "    \"Thanks for the great dinner last night!\"\n",
    "]\n",
    "\n",
    "print(\"Testing the saved model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for message in test_messages:\n",
    "    # Preprocess the message\n",
    "    processed = preprocess_text(message)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = loaded_vectorizer.transform([processed])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(features)[0]\n",
    "    probability = loaded_model.predict_proba(features)[0]\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    label = 'SPAM' if prediction == 1 else 'HAM'\n",
    "    \n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Prediction: {label} (Confidence: {confidence:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with Flask Backend\n",
    "\n",
    "The model and vectorizer are now saved and ready to be used in the Flask backend. The `SpamDetector` class in the Flask app will automatically load these files and use them for predictions.\n",
    "\n",
    "### Key Files Created:\n",
    "- `spam_model.pkl`: The trained machine learning model\n",
    "- `vectorizer.pkl`: The TF-IDF vectorizer for text preprocessing\n",
    "- `model_metadata.json`: Metadata about the model for tracking\n",
    "\n",
    "### Usage in Flask:\n",
    "The Flask backend will load these files automatically when the `SpamDetector` class is instantiated, and use them to make real-time predictions on SMS messages submitted through the API.\n",
    "\n",
    "### Model Performance:\n",
    "- **Accuracy**: {best_accuracy:.4f}\n",
    "- **Model Type**: {best_model_name}\n",
    "- **Features**: {X_train_tfidf.shape[1]} TF-IDF features\n",
    "\n",
    "The model is now ready for production use in your SMS Guard application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
