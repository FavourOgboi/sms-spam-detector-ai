{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Detection Model - Complete Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for creating, training, and integrating an SMS spam detection model with the Flask backend.\n",
    "\n",
    "## What you'll learn:\n",
    "1. How to create and preprocess a dataset\n",
    "2. How to train multiple ML models and compare them\n",
    "3. How to save the trained model for use in Flask\n",
    "4. How the Flask backend loads and uses the model\n",
    "5. How to test the integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Dataset\n",
    "\n",
    "In a real project, you would use a dataset like the SMS Spam Collection. For this demo, we'll create a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample spam and ham messages\n",
    "spam_messages = [\n",
    "    \"FREE! Win a ¬£1000 cash prize! Text WIN to 12345 now!\",\n",
    "    \"URGENT! Your account will be closed. Click here immediately!\",\n",
    "    \"Congratulations! You've won a lottery! Call now to claim your prize!\",\n",
    "    \"Limited time offer! Get 50% off on all products. Buy now!\",\n",
    "    \"WINNER! You have been selected for a special reward. Claim now!\",\n",
    "    \"Free entry to win a brand new iPhone! Text IPHONE to 54321\",\n",
    "    \"Your loan has been approved! Get cash now with no credit check!\",\n",
    "    \"STOP! You owe money. Pay now or face legal action!\",\n",
    "    \"Amazing deal! Buy one get one free! Limited time only!\",\n",
    "    \"You have won $10000! Click this link to claim your money!\",\n",
    "    \"Free ringtones! Text RING to 12345 for unlimited downloads!\",\n",
    "    \"Urgent: Your bank account has been compromised. Verify now!\",\n",
    "    \"Get rich quick! Make $5000 per week working from home!\",\n",
    "    \"Final notice: Your subscription will expire. Renew now!\",\n",
    "    \"Exclusive offer just for you! 90% discount on luxury items!\"\n",
    "] * 15  # Multiply to get more samples\n",
    "\n",
    "ham_messages = [\n",
    "    \"Hi, how are you doing today?\",\n",
    "    \"Can you pick up some milk on your way home?\",\n",
    "    \"Thanks for the great dinner last night!\",\n",
    "    \"Meeting is scheduled for 3 PM tomorrow\",\n",
    "    \"Happy birthday! Hope you have a wonderful day!\",\n",
    "    \"Don't forget about the doctor's appointment\",\n",
    "    \"The weather is really nice today, isn't it?\",\n",
    "    \"I'll be running a bit late for our meeting\",\n",
    "    \"Could you send me the report when you get a chance?\",\n",
    "    \"Let's grab lunch together this weekend\",\n",
    "    \"The project deadline has been extended to next week\",\n",
    "    \"I really enjoyed the movie we watched yesterday\",\n",
    "    \"Please call me when you get this message\",\n",
    "    \"The package should arrive by Friday\",\n",
    "    \"Looking forward to seeing you at the party!\"\n",
    "] * 15  # Multiply to get more samples\n",
    "\n",
    "# Create DataFrame\n",
    "messages = spam_messages + ham_messages\n",
    "labels = ['spam'] * len(spam_messages) + ['ham'] * len(ham_messages)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'message': messages,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} samples\")\n",
    "print(f\"Spam messages: {len(df[df['label'] == 'spam'])}\")\n",
    "print(f\"Ham messages: {len(df[df['label'] == 'ham'])}\")\n",
    "print(\"\\nFirst few samples:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "Clean and prepare the text data for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess SMS text for machine learning\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep letters, numbers, and spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_message'] = df['message'].apply(preprocess_text)\n",
    "\n",
    "# Convert labels to binary (0 for ham, 1 for spam)\n",
    "df['label_binary'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(\"Preprocessing completed!\")\n",
    "print(\"\\nExample of preprocessing:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df['message'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_message'].iloc[i]}\")\n",
    "    print(f\"Label: {df['label'].iloc[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Multiple Models\n",
    "\n",
    "We'll train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = df['processed_message']\n",
    "y = df['label_binary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training set spam ratio: {y_train.mean():.3f}\")\n",
    "print(f\"Test set spam ratio: {y_test.mean():.3f}\")\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nTF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Find the best model\n",
    "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_accuracy = model_results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} with accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Models', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix for the best model\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Model for Flask Backend\n",
    "\n",
    "This is the crucial step - saving the model so Flask can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = '../backend/ml_model/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model and vectorizer\n",
    "model_path = os.path.join(models_dir, 'spam_model.pkl')\n",
    "vectorizer_path = os.path.join(models_dir, 'vectorizer.pkl')\n",
    "\n",
    "joblib.dump(best_model, model_path)\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "print(f\"‚úÖ Vectorizer saved to: {vectorizer_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'accuracy': float(best_accuracy),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'features': int(X_train_tfidf.shape[1]),\n",
    "    'training_samples': int(len(X_train)),\n",
    "    'test_samples': int(len(X_test)),\n",
    "    'vectorizer_params': vectorizer.get_params()\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
    "print(\"\\nüéâ Model is ready for Flask backend!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Saved Model\n",
    "\n",
    "Let's verify that our saved model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model and test it\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "# Test with sample messages\n",
    "test_messages = [\n",
    "    \"Hi, how are you doing today?\",\n",
    "    \"FREE! Win a ¬£1000 cash prize! Text WIN to 12345 now!\",\n",
    "    \"Can you pick up some milk on your way home?\",\n",
    "    \"URGENT! Your account will be closed. Click here immediately!\",\n",
    "    \"Thanks for the great dinner last night!\",\n",
    "    \"Congratulations! You've won a lottery prize!\",\n",
    "    \"Meeting is scheduled for 3 PM tomorrow\"\n",
    "]\n",
    "\n",
    "print(\"Testing the saved model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for message in test_messages:\n",
    "    # Preprocess the message\n",
    "    processed = preprocess_text(message)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = loaded_vectorizer.transform([processed])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(features)[0]\n",
    "    probability = loaded_model.predict_proba(features)[0]\n",
    "    confidence = max(probability)\n",
    "    \n",
    "    label = 'üö® SPAM' if prediction == 1 else '‚úÖ HAM'\n",
    "    \n",
    "    print(f\"Message: {message}\")\n",
    "    print(f\"Prediction: {label} (Confidence: {confidence:.3f})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: How Flask Uses This Model\n",
    "\n",
    "Here's how the Flask backend loads and uses your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is similar to what happens in the Flask backend\n",
    "print(\"Flask Backend Integration Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate the SpamDetector class from Flask\n",
    "class FlaskSpamDetector:\n",
    "    def __init__(self, model_path, vectorizer_path):\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.vectorizer = joblib.load(vectorizer_path)\n",
    "        print(\"‚úÖ Model loaded in Flask backend\")\n",
    "    \n",
    "    def predict(self, message):\n",
    "        # Preprocess\n",
    "        processed = preprocess_text(message)\n",
    "        \n",
    "        # Vectorize\n",
    "        features = self.vectorizer.transform([processed])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(features)[0]\n",
    "        probabilities = self.model.predict_proba(features)[0]\n",
    "        confidence = max(probabilities)\n",
    "        \n",
    "        return {\n",
    "            'prediction': 'spam' if prediction == 1 else 'ham',\n",
    "            'confidence': float(confidence)\n",
    "        }\n",
    "\n",
    "# Create detector instance (like Flask does)\n",
    "flask_detector = FlaskSpamDetector(model_path, vectorizer_path)\n",
    "\n",
    "# Test prediction (like API endpoint does)\n",
    "test_message = \"FREE! Win money now! Click here!\"\n",
    "result = flask_detector.predict(test_message)\n",
    "\n",
    "print(f\"\\nAPI Request: POST /api/predict\")\n",
    "print(f\"Message: {test_message}\")\n",
    "print(f\"API Response: {result}\")\n",
    "print(\"\\n‚úÖ This is exactly how your Flask backend will work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully:\n",
    "\n",
    "1. ‚úÖ Created and preprocessed a dataset\n",
    "2. ‚úÖ Trained multiple machine learning models\n",
    "3. ‚úÖ Selected the best performing model\n",
    "4. ‚úÖ Saved the model for Flask integration\n",
    "5. ‚úÖ Tested the saved model\n",
    "6. ‚úÖ Understood how Flask will use the model\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Start your Flask backend:**\n",
    "   ```bash\n",
    "   cd ../backend\n",
    "   python run.py\n",
    "   ```\n",
    "\n",
    "2. **Test the API:**\n",
    "   ```bash\n",
    "   python test_api.py\n",
    "   ```\n",
    "\n",
    "3. **Connect your React frontend** to the Flask backend by updating the API base URL\n",
    "\n",
    "### Model Performance:\n",
    "- **Best Model:** {best_model_name}\n",
    "- **Accuracy:** {best_accuracy:.4f}\n",
    "- **Features:** {X_train_tfidf.shape[1]} TF-IDF features\n",
    "\n",
    "Your SMS Guard application is now ready with a fully functional machine learning backend! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
